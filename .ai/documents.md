Traktuję LLM jak nowy sposób wpisywania kodu
Zawsze samodzielnie wskazuję kontekst rozmowy
Utrzymuję kontekst rozmowy tak krótkim, jak to możliwe
Korzystam niemal wyłącznie z własnej wersji dokumentacji
Tworząc pliki specyfikacji dbam o niski poziom "szumu"
Adresuję wyłącznie wąskie, jasno sprecyzowane problemy
Unikam rozbudowanych jednorazowych zmian, które trudno zrozumieć
Informacje na temat kodu wczytuję poprzez zadawanie pytań
Przełączam się między modelami w trakcie rozmowy
Korzystam z różnych modeli do rozwiązań tych samych problemów
Korzystam zarówno z CLI jak i IDE w celu zestawiania możliwości
Testuję nowe modele na najtrudniejszych problemach i bugach
Unikam naprawiania błędów poprzez przesyłanie treści błędów
Wracam do wcześniejszych etapów konwersacji
Rozmawiam z modelem o nowych dla mnie narzędziach
Proszę model o zadawanie mi pytań
Zadaję modelowi pytania, aby określić zakres jego bazowej wiedzy
Pytam model o pytania, które zadałby sobie na moim miejscu
Dedykuję oddzielne rozmowy na optymalizację bądź refactor
Proszę o parafrazę moich słów w celu ich weryfikacji
Korzystam z wiedzy modelu poprzez zadawanie pytań
Ograniczam długość wypowiedzi modelu poprzez prompt
Proszę model o wygenerowanie github-issue-like dokumentu
Korzystam z głosowych integracji do rozmów do omawiania problemów
Przy rozwiązywaniu problemów zadaję otwarte, niesugerujące pytania
Tam gdzie to możliwe dostarczam dynamicznie generowane pliki logów
Zwykle najpierw buduję, potem opisuję i optymalizuję
Korzystam z modelu jako wsparcia do uchwycenia dużego problemu
Rozmawiam z modelem na temat technik optymalizacji kodu i promptów
Omawiam decyzje projektowe oraz UI z różnymi modelami
Nie poruszam się daleko poza zakresem mojej kompetencji...
...chyba że w celu eksploracji nowych technologii i narzędzi

MCP ref.tools
